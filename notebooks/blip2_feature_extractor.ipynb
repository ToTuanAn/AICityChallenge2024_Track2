{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 22:32:46.146499: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-06 22:32:46.174926: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-06 22:32:46.304982: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-06 22:32:47.199480: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, Blip2VisionModel\n",
    "\n",
    "raw_image = Image.open(\"./images/demo.jpg\").convert('RGB')\n",
    "\n",
    "question = \"how many dogs are in the picture?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLIP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\", cache_dir=\"./cache\")\n",
    "\n",
    "inputs = processor(raw_image, question, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float32, device_map=\"cpu\", cache_dir='./cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model.vision_model.save_pretrained(\"./blip2-opt-2.7b-vision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Push vision model to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model.vision_model.push_to_hub(\"tmnam20/blip2-opt-2.7b-vision\", private=True, token=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Push processor to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.push_to_hub(\"tmnam20/blip2-opt-2.7b-vision\", private=True, token=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vision models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Blip2VisionModel(\n",
       "  (embeddings): Blip2VisionEmbeddings(\n",
       "    (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
       "  )\n",
       "  (encoder): Blip2Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-38): 39 x Blip2EncoderLayer(\n",
       "        (self_attn): Blip2Attention(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "          (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Blip2MLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "          (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (post_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_processor = Blip2Processor.from_pretrained(\"tmnam20/blip2-opt-2.7b-vision\",torch_dtype=torch.float32, cache_dir=\"./cache\")\n",
    "vision_model = Blip2VisionModel.from_pretrained(\"tmnam20/blip2-opt-2.7b-vision\", device_map='cpu', torch_dtype=torch.float32, cache_dir=\"./cache\")\n",
    "vision_model = vision_model.eval()\n",
    "vision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = vision_processor(raw_image, question, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input pixels shape: torch.Size([1, 3, 224, 224])\n",
      "Vision embeddings shape: torch.Size([1, 257, 1408])\n"
     ]
    }
   ],
   "source": [
    "input_pixels = inputs['pixel_values']\n",
    "with torch.no_grad():\n",
    "    vision_embeddings = vision_model(input_pixels)\n",
    "print(f'Input pixels shape: {input_pixels.shape}')\n",
    "print(f'Vision embeddings shape: {vision_embeddings.last_hidden_state.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before extend: torch.Size([1, 3, 224, 224])\n",
      "Shape after extend: torch.Size([1, 1, 3, 224, 224])\n",
      "Shape after repeat: torch.Size([2, 4, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "input_pixels = inputs['pixel_values']\n",
    "print(f'Shape before extend: {input_pixels.shape}') # [batch_size, 3, height, width]\n",
    "# extend to [batch_size, 1, 3, height, width]\n",
    "input_pixels = input_pixels.unsqueeze(1)\n",
    "print(f'Shape after extend: {input_pixels.shape}')\n",
    "input_pixels = input_pixels.repeat(2, 4, 1, 1, 1)\n",
    "print(f'Shape after repeat: {input_pixels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "vision_model.train()\n",
    "\n",
    "encoded = []\n",
    "for batch_idx in range(input_pixels.size(0)):\n",
    "    print(batch_idx)\n",
    "    item = input_pixels[batch_idx].contiguous()\n",
    "    encoded_item = vision_model(pixel_values=item)\n",
    "    encoded.append(encoded_item.last_hidden_state)\n",
    "encoded = torch.stack(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check backward compatibility of vision models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape of encoded: {encoded.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [batch_size, num_frames, num_pathces, hidden_size]\n",
    "# get the first token in num_patches\n",
    "encoded = encoded[:, :, 0, :]\n",
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_layer = nn.Linear(vision_model.config.hidden_size, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_encoded_features = projection_layer(encoded)\n",
    "print(f'Projected image features shape = {scaled_encoded_features.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: [[0.0123  0.962   0.02563]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "image = preprocess(Image.open(\"images/demo.jpg\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2162f84ab2490d9376bba54110af05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", cache=\"./cache\")\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", device_map=\"cpu\", cache_dir=\"./cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "class LlamaVideo(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vision_encoder,\n",
    "        tokenizer,\n",
    "        language_model,\n",
    "        freeze_vision_encoder=True,\n",
    "        freeze_language_model=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.lm = language_model\n",
    "        \n",
    "        vision_encoder_output_dim = vision_encoder.config.hidden_size\n",
    "        lm_output_dim = language_model.config.hidden_size\n",
    "        self.vision_projection = nn.Linear(vision_encoder_output_dim, lm_output_dim)\n",
    "        if freeze_vision_encoder:\n",
    "            self.freeze_vision_encoder()\n",
    "        if freeze_language_model:\n",
    "            self.freeze_language_model()\n",
    "        \n",
    "    def freeze_vision_encoder(self):\n",
    "        for param in self.vision_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"Freeze vision encoder\")\n",
    "        \n",
    "    def freeze_language_model(self):\n",
    "        for param in self.lm.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"Freeze language model\")\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        frames: torch.Tensor,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        \"\"\"Forward pass of VideoLlama\n",
    "\n",
    "        Args:\n",
    "            frames (torch.Tensor): Input in shape (batch_size, seq_len, channels, width, height) or (batch_size, seq_len, features)\n",
    "            tokenized_input (dict): input dictionary of the prompts\n",
    "        \"\"\"\n",
    "        if len(frames.size()) == 5: # [batch_size, seq_length, channels, width, height]\n",
    "            frame_features = []\n",
    "            for batch_idx in range(frames.size(0)):\n",
    "                cur_frames = frames[batch_idx].contiguous()\n",
    "                frame_features.append(\n",
    "                    self.vision_encoder(cur_frames).last_hidden_state[:, 0, :]\n",
    "                )\n",
    "            frame_features = torch.stack(frame_features)\n",
    "            frame_features = self.vision_projection(frame_features)\n",
    "        elif len(frames.size()) == 3: # [batch_size, seq_length, features]\n",
    "            frame_features = self.vision_projection(frames)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input shape. Must be either (batch_size, seq_len, channels, width, height) or (batch_size, seq_len, features)\")\n",
    "        \n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"Only one of input_ids or inputs_embeds can be populated\")\n",
    "        \n",
    "        if input_ids is not None:\n",
    "            embedding_layer = self.lm.get_input_embeddings()\n",
    "            text_features = embedding_layer(input_ids)\n",
    "            input_features = torch.cat([frame_features, text_features], dim=1)\n",
    "        elif inputs_embeds is not None:\n",
    "            text_features = inputs_embeds\n",
    "            input_features = torch.cat([frame_features, text_features], dim=1)\n",
    "        else:\n",
    "            input_features = frame_features\n",
    "\n",
    "        video_feature_len = frame_features.size(1)\n",
    "        if labels is not None:\n",
    "            padding = torch.full((labels.size(0), video_feature_len), -100, dtype=torch.long, device=labels.device)\n",
    "            labels = torch.cat([padding, labels], dim=1)\n",
    "        attention_mask = torch.cat([torch.ones((input_features.size(0), video_feature_len), dtype=torch.long, device=input_features.device), attention_mask], dim=1)\n",
    "\n",
    "        outputs = self.lm(\n",
    "            input_ids=None,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=input_features,\n",
    "            labels=labels,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(\n",
    "        vision_encoder_name_or_path: str,\n",
    "        language_model_name_or_path: str,\n",
    "        vision_projection_name_or_path: str,\n",
    "        **kwargs\n",
    "    ):\n",
    "        vision_encoder = Blip2VisionModel.from_pretrained(vision_encoder_name_or_path, **kwargs)\n",
    "        language_model = LlamaForCausalLM.from_pretrained(language_model_name_or_path, **kwargs)\n",
    "        model = LlamaVideo(vision_encoder, language_model, **kwargs)\n",
    "        if vision_projection_name_or_path is not None:\n",
    "            model.vision_projection.load_state_dict(torch.load(vision_projection_name_or_path))\n",
    "        return model\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        video,\n",
    "        input_tokenized,\n",
    "        use_nucleus_sampling=False,\n",
    "        num_beams=4,\n",
    "        max_length=256,\n",
    "        min_length=1,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.0,\n",
    "        length_penalty=1.0,\n",
    "        num_captions=1,\n",
    "        temperature=1,\n",
    "    ):\n",
    "        if len(video.size()) == 5:\n",
    "            frame_features = []\n",
    "            for batch_idx in range(video.size(0)):\n",
    "                item = video[batch_idx].contiguous()\n",
    "                encoded_item = self.vision_encoder(pixel_values=item)\n",
    "                frame_features.append(encoded_item.last_hidden_state[:, 0, :])\n",
    "            frame_features = torch.stack(frame_features)\n",
    "            frame_features = self.vision_projection(frame_features)\n",
    "        elif len(video.size()) == 3:\n",
    "            frame_features = self.vision_projection(video)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input shape. Must be either (batch_size, seq_len, channels, width, height) or (batch_size, seq_len, features)\")\n",
    "        \n",
    "        video_feature_len = frame_features.size(1)\n",
    "        attention_mask = torch.ones((frame_features.size(0), video_feature_len), dtype=torch.long, device=frame_features.device)\n",
    "        \n",
    "        outputs = self.lm.generate(\n",
    "            input_ids=None,\n",
    "            inputs_embeds=frame_features,\n",
    "            attention_mask=attention_mask,\n",
    "            do_sample=use_nucleus_sampling,\n",
    "            top_p=top_p,\n",
    "            temperature=temperature,\n",
    "            num_beams=num_beams,\n",
    "            max_new_tokens=max_length,\n",
    "            min_length=min_length,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            length_penalty=length_penalty,\n",
    "            num_return_sequences=num_captions,\n",
    "        )\n",
    "        output_text = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        \n",
    "        return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   920,  1784, 26361,   526,   297,   278,  7623, 29973],\n",
       "        [    1,   825,  2927,   338,   278, 11203, 29973,     2,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0]]), 'labels': tensor([[    1,   920,  1784, 26361,   526,   297,   278,  7623, 29973],\n",
       "        [    1,   825,  2927,   338,   278, 11203, 29973,     2,     2]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question1 = \"how many dogs are in the picture?\"\n",
    "question2 = \"what color is the dog?\"\n",
    "\n",
    "model_inputs = llama_tokenizer(\n",
    "    [question1, question2], \n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ")\n",
    "model_inputs['labels'] = model_inputs['input_ids'].clone()\n",
    "model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeze vision encoder\n",
      "Freeze language model\n"
     ]
    }
   ],
   "source": [
    "llama_video = LlamaVideo(\n",
    "    vision_encoder=vision_model,\n",
    "    tokenizer=llama_tokenizer,\n",
    "    language_model=llama_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_output = llama_video(\n",
    "    frames=input_pixels,\n",
    "    **model_inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'logits', 'past_key_values'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 13, 32000]), tensor(6.0217, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_output.logits.size(), tmp_output.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': device(type='cpu')}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_video.lm.hf_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhnam/miniconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "generated = llama_video.generate(\n",
    "    video=input_pixels,\n",
    "    input_tokenized=model_inputs,\n",
    "    use_nucleus_sampling=False,\n",
    "    num_beams=4,\n",
    "    max_length=4,\n",
    "    min_length=1,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.0,\n",
    "    length_penalty=1.0,\n",
    "    num_captions=1,\n",
    "    temperature=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OOOO', 'OOOO']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
