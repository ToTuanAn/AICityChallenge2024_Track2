{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "training_samples_path = \"training_samples.json\"\n",
    "training_samples_data = json.load(open(training_samples_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video_name': '20230707_12_SN17_T1_vehicle_view',\n",
       " 'video_path': '/Users/minhnam/Desktop/playground/aic2024-t2/datasets/videos/train/20230707_12_SN17_T1/vehicle_view/20230707_12_SN17_T1_vehicle_view.mp4',\n",
       " 'timestamp': [0, 32],\n",
       " 'speed': 10.0,\n",
       " 'sentence': \"The vehicle is positioned diagonally to the right in front of the pedestrian, at a close distance. The pedestrian is visible within the vehicle's field of view. The vehicle is going straight ahead at a speed of 10 km/h. Meanwhile, in the environment, there is a male pedestrian in his 30s, standing at a height of 170 cm. He is wearing a black T-shirt and black slacks. The weather is clear with bright lighting, and the road surface is dry and level. The road is a residential road with two-way traffic and does not have sidewalks on both sides. There are no roadside strips, but the street lights are on. Overall, the vehicle is in a normal traffic situation, with clear visibility of the pedestrian and suitable road conditions for its speed and direction.\",\n",
       " 'event_index': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_point = training_samples_data[0]\n",
    "data_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = data_point[\"video_path\"]\n",
    "speed = data_point[\"speed\"]\n",
    "start, end = data_point[\"timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open video and read frame by frame\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frames, idx = [], 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        frames.append(frame)\n",
    "        idx += 1\n",
    "    else:\n",
    "        break\n",
    "frames = frames[start:end]\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_optical_flow(video_path: str, start: int, end: int):\n",
    "    \"\"\"Source: https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_video/py_lucas_kanade/py_lucas_kanade.html\n",
    "\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    ret, frame1 = cap.read()\n",
    "    prvs = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)\n",
    "    hsv = np.zeros_like(frame1)\n",
    "    hsv[...,1] = 255\n",
    "    \n",
    "    frames, idx = [], 1\n",
    "    \n",
    "    while(cap.isOpened()):\n",
    "        ret, frame2 = cap.read()\n",
    "        if not ret or idx > end:\n",
    "            break\n",
    "        \n",
    "        next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)\n",
    "        if idx >= start and idx <= end:\n",
    "            flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "            # flow is in shape (h, w, 2)\n",
    "\n",
    "            # convert flow to rgb\n",
    "            mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
    "            hsv[...,0] = ang*180/np.pi/2\n",
    "            hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n",
    "            bgr = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)\n",
    "            \n",
    "            # save the frame\n",
    "            frames.append(bgr)\n",
    "        \n",
    "        prvs = next\n",
    "        idx += 1\n",
    "    \n",
    "    cap.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_flow_frames = extract_optical_flow(video_path, start, end)\n",
    "os.makedirs(\"opt_flow_frames\", exist_ok=True)\n",
    "for idx, frame in enumerate(opt_flow_frames):\n",
    "    cv2.imwrite(f\"opt_flow_frames/{idx}.jpg\", frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 490/490 [2:20:33<00:00, 17.21s/it]  \n"
     ]
    }
   ],
   "source": [
    "optical_flow_save_dir = \"dataset/images\"\n",
    "os.makedirs(optical_flow_save_dir, exist_ok=True)\n",
    "optical_flow_labels = {}\n",
    "\n",
    "os.makedirs(optical_flow_save_dir, exist_ok=True)\n",
    "for data_point in tqdm(training_samples_data):\n",
    "    video_name = data_point[\"video_name\"]\n",
    "    video_path = data_point[\"video_path\"]\n",
    "    speed = int(data_point[\"speed\"])\n",
    "    start, end = data_point[\"timestamp\"]\n",
    "    event_index = data_point[\"event_index\"]\n",
    "    \n",
    "    opt_flow_frames = extract_optical_flow(video_path, start, end)\n",
    "    \n",
    "    video_optical_flow_save_dir = os.path.join(optical_flow_save_dir, video_name)\n",
    "    os.makedirs(video_optical_flow_save_dir, exist_ok=True)\n",
    "    for idx, frame in enumerate(opt_flow_frames):\n",
    "        filename = f\"{video_name}_event{event_index}_{idx}.jpg\"\n",
    "        cv2.imwrite(os.path.join(video_optical_flow_save_dir, filename), frame)\n",
    "        optical_flow_labels[filename] = {\n",
    "            \"speed\": speed,\n",
    "            \"event_index\": event_index,\n",
    "            \"video_name\": video_name\n",
    "        }\n",
    "\n",
    "with open(\"./dataset/labels.json\", \"w\") as f:\n",
    "    json.dump(optical_flow_labels, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 13391,\n",
       "         10.0: 7245,\n",
       "         20.0: 6978,\n",
       "         5.0: 3732,\n",
       "         30.0: 1924,\n",
       "         15.0: 782,\n",
       "         25.0: 136})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_speeds = Counter([v[\"speed\"] for k, v in optical_flow_labels.items()])\n",
    "cnt_speeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SpeedPredictionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images_dir, labels_path, transform=None):\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        labels_data = json.load(open(labels_path))\n",
    "        for image_filename, data in labels_data.items():\n",
    "            speed = data['speed']\n",
    "            self.labels.append(speed)\n",
    "\n",
    "            video_name = data['video_name']\n",
    "            image_path = os.path.join(images_dir, video_name, image_filename)\n",
    "            self.image_paths.append(image_path)\n",
    "        \n",
    "        label_set = set(self.labels)\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(label_set)}\n",
    "        self.idx_to_label = {idx: label for label, idx in self.label_to_idx.items()}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path)\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(self.label_to_idx[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SpeedPredictionDataset(\n",
    "    images_dir='./dataset/images/',\n",
    "    labels_path='./dataset/labels.json',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1920x1080>, tensor(2))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
